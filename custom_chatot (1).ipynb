{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbi4e10CZIf3"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv langchain pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQtRS1IMaT4E"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJZXVmT2a5EX"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0qZZ8CUZenM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"GOOGLE_API_KEY\"\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"PINECONE_API_KEY\"\n",
        "os.environ[\"PINECONE_ENV\"] = \"PINECONE_ENV\"\n",
        "os.environ[\"PINECONE_INDEX_NAME\"] = \"PINECONE_INDEX_NAME\"\n",
        "os.environ[\"RAPID_API_KEY\"] = \"RAPID_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC_4dJ0vcGi9",
        "outputId": "fb39f828-7307-47ff-eae9-8a8a862b089f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¤– Ask me anything! Type 'exit' to end the conversation.\n",
            "\n",
            "ðŸ§‘ You: 2+2?\n",
            "ðŸ¤– Gemini: 4\n",
            "ðŸ§‘ You: What's the cost of living in NewYork?\n",
            "ðŸ›  Fetching cost of living...\n",
            "The cost of living in NewYork? is moderate to high depending on the neighborhood.\n",
            "ðŸ§‘ You: 3-1?\n",
            "ðŸ¤– Gemini: 2\n",
            "ðŸ§‘ You: what i asked before?\n",
            "ðŸ¤– Gemini: Previously, you asked:\n",
            "\n",
            "* \"previous answer-2?\"\n",
            "* \"4-5?\"\n",
            "* \"2+2?\" (twice)\n",
            "ðŸ§‘ You: exit\n",
            "ðŸ‘‹ Goodbye!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from pinecone import Pinecone as PineconeClient\n",
        "import requests\n",
        "\n",
        "# Load env variables\n",
        "load_dotenv()\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
        "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
        "RAPID_API_KEY = os.getenv(\"RAPID_API_KEY\")\n",
        "\n",
        "# Ensure Gemini API key is set\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "# Embeddings using Gemini\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "# Pinecone setup\n",
        "# Make sure PINECONE_API_KEY is your actual API key\n",
        "pinecone = PineconeClient(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
        "# Make sure PINECONE_INDEX_NAME is your actual index name\n",
        "index = pinecone.Index(PINECONE_INDEX_NAME)\n",
        "\n",
        "# Vector store\n",
        "vectorstore = PineconeVectorStore(index=index, embedding=embeddings, text_key=\"text\")\n",
        "\n",
        "# LLM (Gemini)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2)\n",
        "\n",
        "# Memory\n",
        "memory = ConversationBufferMemory(\n",
        "    return_messages=True,\n",
        "    input_key=\"question\",\n",
        "    memory_key=\"chat_history\"\n",
        ")\n",
        "# Function to fetch tech jobs in a city (using RapidAPI or another job API)\n",
        "def get_tech_jobs(city):\n",
        "    url = \"https://jsearch.p.rapidapi.com/search\"\n",
        "    querystring = {\"query\": f\"tech jobs in {city}\", \"page\": \"1\"}\n",
        "    headers = {\n",
        "        \"X-RapidAPI-Key\": RAPID_API_KEY,\n",
        "        \"X-RapidAPI-Host\": \"jsearch.p.rapidapi.com\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers, params=querystring)\n",
        "    data = response.json()\n",
        "    jobs = data.get(\"data\", [])\n",
        "    return \"\\n\".join([f\"{job['job_title']} at {job['employer_name']}\" for job in jobs[:3]]) if jobs else \"No jobs found.\"\n",
        "\n",
        "def get_cost_of_living(city):\n",
        "    # Placeholder for Numbeo or static cost-of-living data\n",
        "    return f\"The cost of living in {city} is moderate to high depending on the neighborhood.\"\n",
        "\n",
        "\n",
        "# QA chain\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    memory=memory,\n",
        ")\n",
        "# Save previous important user queries & answers as embedded docs\n",
        "\n",
        "# ðŸ’¬ Interactive loop\n",
        "print(\"ðŸ¤– Ask me anything! Type 'exit' to end the conversation.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"ðŸ§‘ You: \")\n",
        "\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"ðŸ‘‹ Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # âœ… Check for real-time tool usage\n",
        "    if \"tech jobs\" in user_input.lower():\n",
        "        city = user_input.split(\"in\")[-1].strip() if \"in\" in user_input else \"San Francisco\"\n",
        "        print(\"ðŸ›  Fetching tech jobs...\")\n",
        "        print(get_tech_jobs(city))\n",
        "        continue\n",
        "\n",
        "    elif \"cost of living\" in user_input.lower():\n",
        "        city = user_input.split(\"in\")[-1].strip() if \"in\" in user_input else \"New York\"\n",
        "        print(\"ðŸ›  Fetching cost of living...\")\n",
        "        print(get_cost_of_living(city))\n",
        "        continue\n",
        "\n",
        "    # âœ… Inject long-term memory into context\n",
        "    past_memories = vectorstore.similarity_search(user_input, k=3, filter={\"type\": \"memory\"})\n",
        "    if past_memories:\n",
        "        print(\"ðŸ“š Recalling related past chats...\")\n",
        "        for mem in past_memories:\n",
        "            print(\"ðŸ§ \", mem.page_content)\n",
        "\n",
        "    # ðŸ§  Ask Gemini\n",
        "    response = qa_chain.invoke({\"question\": user_input})\n",
        "    print(f\"ðŸ¤– Gemini: {response['answer']}\")\n",
        "\n",
        "    important_memory = f\"User asked: {user_input}, Gemini said: {response['answer']}\"\n",
        "vectorstore.add_texts([important_memory], metadata={\"type\": \"memory\", \"user\": \"u1\"})\n",
        "past_memories = vectorstore.similarity_search(\"tech jobs\", k=5, filter={\"type\": \"memory\", \"user\": \"u1\"})\n",
        "for memory in past_memories:\n",
        "    print(memory.page_content)\n",
        "\n",
        "    # âœ… Store to long-term memory\n",
        "    memory_log = f\"User asked: {user_input}\\nGemini said: {response['answer']}\"\n",
        "    vectorstore.add_texts([memory_log], metadata={\"type\": \"memory\", \"user\": \"u1\"})\n",
        "\n",
        "    # Optional: print session memory\n",
        "    print(\"\\nðŸ§  Chat so far:\")\n",
        "    for msg in memory.chat_memory.messages:\n",
        "        role = \"You\" if msg.type == \"human\" else \"Gemini\"\n",
        "        print(f\"{role}: {msg.content}\")\n",
        "    print(\"-\" * 50)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}